NAME: Michael Wu
EMAIL: cheeserules43@gmail.com
ID: 404751542

Question 2.1.1

Why does it take many iterations before errors are seen?

The scheduler will only interrupt a thread a few times, and if a thread is not
interrupted between the load and the store of the counter, then it will execute
the critical section correctly and not result in an error. With more iterations
there will be more interrupts, raising the chance that a thread will be
interrupted during the critical section.

Why does a significantly smaller number of iterations so seldom fail?

Because for a small number of iterations, the probability that the race
condition results in an error is smaller since there are less opportunities for
the threads to interfere with each other. There are fewer interrupts which
lowers the likelihood that a thread will be interrupted during the critical
section.

Question 2.1.2

Why are the --yield runs so much slower?

Because yielding and switching threads incurs an overhead that reduces the
overall performance. Yielding for every iteration results in a large slowdown in
performance. Additionally real time is spent running threads not owned by our
process since the yield may result in another process being scheduled.

Where is the additional time going?

The additional time is spent storing the registers and loading new registers to
run a different thread each yield, and possibly running other processes.

Is it possible to get valid per-operation timings if we are using the --yield
option?

Yes, we simply use the cpu-time clock instead of the real clock to avoid
counting other processes' time in the calculation. I have already made my
default implementation use the cpu-time clock. This is assuming that we consider
the yield to be part of the add operation. There is no way to separate the yield
part from the calculation. Any measurement of individual operations such as a
single addition statement would incur a high overhead, resulting in inaccurate
measurements.

Question 2.1.3

Why does the average cost per operation drop with increasing iterations?

Because the load and store operations themselves are very short, when we run few
operations a large part of their cost is the overhead of making and managing
threads. As we increase the iterations the cost of making and managing threads
is amortized over the number of iterations.

If the cost per iteration is a function of the number of iterations, how do we
know how many iterations to run (or what the "correct" cost is)?

Simply run a high number of operations. We should expect that as we increase the
number of operations the average cost per operation approaches the true cost per
operation. The overhead should be minimized with a high number of operations,
leading to a flattening of the average cost per iteration vs iteration
graph. Thus the correct cost is the value that the graph asymptotically
approaches.

Question 2.1.4

Why do all of the options perform similarly for low numbers of threads?

Because the synchronization method does not slow down the program
significantly. For a single thread the locks do not do anything, so the
execution is not slowed down other than by the overhead of the synchronization
method. With two threads running, only one thread may be blocked at a time which
means that the execution is slowed down only sometimes. Any particular thread is
expected to be blocked about 50% of the time. Having a thread spin in this case
is about as costly as the overhead of switching to another thread to execute. So
the options perform about the same in this case, but they begin to diverge
after that.

Why do the three protected operations slow down as the number of threads rises?

As the number of threads rises the time that any given thread can expect to be
blocked will rise. With four threads, one thread can expect to be blocked 75% of
the time, with 8 threads, one can expect to be blocked 87.5% of the time, etc.
So more time is spent waiting and switching between threads rather than doing
what we want.

Question 2.2.1

Compare the variation in time per mutex-protected operation vs the number of
threads in Part-1 (adds) and Part-2 (sorted lists).

They both appear to increase the time per operation as the number of threads go
up. However the absolute value of the length adjusted time per operation curve
is smaller, but that is due to the length adjustment. Other than that they look
quite similar.

Comment on the general shapes of the curves, and explain why they have this
shape.

The time per operations appears to increase linearly with the number of
threads. The linear scale occurs because with T threads, any given thread will
only run with efficiency E=1/T percent of the time. Thus the time t_T for any
operation with T threads is scaled to t_T=t_1/E or t_T=t_1*T. Thus the cost per
operation scales linearly with T.

Comment on the relative rates of increase and differences in the shapes of the
curves, and offer an explanation for these differences.

The curve shapes have practically no difference. The relative rates of increase
differ, but that is due to the adjustment for length of the list and the fact
that the two programs are performing different operations.

Question 2.2.2

Compare the variation in time per protected operation vs the number of threads
for list operations protected by Mutex vs Spin locks.

The time per protected operation is generally more for spin locks than mutex
locks. They are about the same for one thread or two threads, but spin locks
result in far greater inefficiency for more than two threads.

Comment on the general shapes of the curves, and explain why they have this shape.

The mutex curve appears relatively straight, this would indicate that the cost
per operation scales with number of threads linearly. The spin lock is also
relatively straight, but has a steeper slope. The linear scale occurs because
with T threads, any given thread will only run with efficiency E=1/T percent of the
time. Thus the time t_T for any operation with T threads is scaled to t_T=t_1/E
or t_T=t_1*T. Thus the cost per operation scales linearly with T.

Comment on the relative rates of increase and differences in the shapes of the
curves, and offer an explanation for these differences.

The spin lock has a steeper slope than the mutex lock. This is because it is
less efficient than the mutex lock because the mutex lock can result in a thread
switching to another one if it does not acquire a lock. The spin lock wastes an
entire time slice per thread instead of switching when the thread is
blocked.Thus less processor time is wasted with a mutex lock. This effect is
more noticeable with more threads.

This submission contains:

lab2_add.c: Source file that runs multiple threads and iterations of adding to
shared counter.

lab2_list.c: Source file that runs multiple threads and iterations of modifying
linked list.

SortedList.h: Header for sorted list implementation.

SortedList.c: Source file for sorted list implementation with yields.

Makefile: Builds project, do make dist to generate all tests and graphs. Default
target only make executables lab2_add and lab2_list.

lab2_add.gp: Plotting script for lab2_add data.

lab2_list.gp: Plotting script for lab2_list data.

lab2_add.csv: lab2_add test results.

lab2_list.csv: lab2_list test results.

lab2_add-1.png: Graph of lab2_add showing that the yield option results in more
failures than without.

lab2_add-2.png: Graph of lab2_add showing that cost per operation is higher
with yields.

lab2_add-3.png: Graph of lab2_add showing that cost per operation decreases with
increasing iterations.

lab2_add-4.png: Graph of lab2_add showing that synchronization options always
results in successful run.

lab2_add-5.png: Graph of lab2_add showing performance of different
synchronization options.

lab2_list-1.png: Graph of lab2_list showing cost per operation with respect to
iterations.

lab2_list-2.png: Graph of lab2_list showing that yield options cause more
failures than without.

lab2_list-3.png: Graph of lab2_list showing that synchronization options result
in sucessful runs.

lab2_list-4.png: Graph of lab2_list showing performance of different
synchronization options.
